{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEcSNKhrotPo"
      },
      "source": [
        "# Getting Started\n",
        "\n",
        "## General Tips\n",
        "In each homework problem, you will implement various autoencoder models and run them on two datasets (dataset 1 and dataset 2). The expected outputs for dataset 1 are already provided to help as a sanity check.\n",
        "\n",
        "Feel free to print whatever output (e.g. debugging code, training code, etc) you want, as the graded submission will be the submitted pdf with images.\n",
        "\n",
        "After you complete the assignment, download all of the image outputted in the results/ folder and upload them to the figure folder in the given latex template.\n",
        "\n",
        "Run the cells below to download and load up the starter code. It may take longer to run since we are using larger datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get to the parent dir of mai_dul repo\n",
        "import os\n",
        "os.chdir('../../')\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# install latest version deepul package\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from deepul.hw2_helper import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAAixSJ1dv7u"
      },
      "source": [
        "# Question 2: VAEs on Images [40pts]\n",
        "In this question, you will train different VAE models on image datasets. Execute the cell below to visualize the two datasets ([SVHN](http://ufldl.stanford.edu/housenumbers/) and CIFAR10 (https://www.cs.toronto.edu/~kriz/cifar.html))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        },
        "id": "gj2CDM5bXBTG",
        "outputId": "a75eeb17-b26a-434d-be97-fa4aeac3a38b"
      },
      "outputs": [],
      "source": [
        "visualize_svhn()\n",
        "visualize_cifar10()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "822YbCb2fsz8"
      },
      "source": [
        "## Part (b) Hierarchical VAE [20pts] - optional\n",
        "\n",
        "In this part, we will explore a simplified version of the hierarchical VAE described in [NVAE](https://arxiv.org/pdf/2007.03898.pdf). We will not implement the full NVAE, but rather use some ideas from the paper to explore how to learn a prior distribution p(z).\n",
        "\n",
        "Implement a hierarchical VAE that follows the following structure.\n",
        "* $z1$ is a 2x2x12 latent vector where p(z1) is the unit Gaussian.\n",
        "    * Learn the approximate posterior $q_\\theta(z|x) = N(z; \\mu_\\theta(x), \\Sigma_\\theta(x))$, where $\\mu_\\theta(x)$ is the mean vector, and $\\Sigma_\\theta(x)$ is a diagonal covariance matrix. I.e., same as a normal VAE, but use a matrix latent rather than a vector. Each dimension is independent.\n",
        "* $z2$ is a 2x2x12 latent vector.\n",
        "    * $p_\\theta(z2|z1)$ is learned, and implemented as a neural network that parameterizes mean (and log std, optionally).\n",
        "    * $q_\\theta(z2|z1,x)$ is also learned. Implement this as a Residual Normal [see NVAE] over the prior $p_\\theta(z2|z1)$.\n",
        "* The decoder should be a function of $z2$ only.\n",
        "\n",
        "Some helpful hints:\n",
        "* Two KL losses should be calculated. The first should match $q_\\theta(z|x)$ to the unit Gaussian. The second should match $q_\\theta(z2|z1,x)$ and $p_\\theta(z2|z1)$, and be taken with respect to $q$.\n",
        "* When calculating the second KL term, utilize the analytic form for the residual normal. When $q_\\theta(z2|z1,x) = N(z2; \\mu_\\theta(z1) + \\Delta \\mu_\\theta(z1,x), \\Sigma_\\theta(z1)) * \\Delta \\Sigma_\\theta(z1,x))$, use the following form: `kl_z2 = -z2_residual_logstd - 0.5 + (torch.exp(2 * z2_residual_logstd) + z2_residual_mu ** 2) * 0.5`\n",
        "* When calculating KL, remember to sum over the dimensions of the latent variable before taking the mean over batch.\n",
        "* For the prior $p_\\theta(z2|z1)$, fix standard deviation to be 1. Learn only the mean. This will help with stability in training.\n",
        "\n",
        "The following network structures may be useful:\n",
        "```\n",
        "conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "transpose_conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "linear(in_dim, out_dim)\n",
        "\n",
        "Encoder\n",
        "        nn.Conv2d(3 + 12, 32, 3, padding=1), # [32, 32, 32]\n",
        "        LayerNorm(32),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 64, 3, stride=2, padding=1), # [64, 16, 16]\n",
        "        LayerNorm(64),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 64, 3, stride=2, padding=1), # [64, 8, 8]\n",
        "        LayerNorm(64),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 64, 3, stride=2, padding=1), # [64, 4, 4]\n",
        "        LayerNorm(64),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 64, 3, stride=2, padding=1), # [64, 2, 2]\n",
        "        LayerNorm(64),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 12*2, 3, padding=1), # [12*2, 2, 2]\n",
        "We assume encoder networks are of the form p(z'|z,x).\n",
        "When learning q(z1), an x of all zeros can be used as input.\n",
        "Upscale z with nearest-neighbor projection before concatenating with x.\n",
        "\n",
        "\n",
        "Decoder\n",
        "        nn.ConvTranspose2d(12, 64, 3, padding=1), # [64, 2, 2]\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(64, 64, 4, stride=2, padding=1), # [64, 4, 4]\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(64, 64, 4, stride=2, padding=1), # [64, 8, 8]\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(64, 64, 4, stride=2, padding=1), # [64, 16, 16]\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1), # [32, 32, 32]\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 3, 3, padding=1), # [3, 32, 32]\n",
        "```\n",
        "\n",
        "\n",
        "**You will provide the following deliverables**\n",
        "\n",
        "\n",
        "1.   Over the course of training, record the average full negative ELBO, reconstruction loss, and KL term of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves.\n",
        "2.   Report the final test set performance of your final model\n",
        "3. 100 samples from your trained VAE\n",
        "4. 50 real-image / reconstruction pairs (for some $x$, encode and then decode)\n",
        "5. 10 interpolations of 10 images from your trained VAE (100 images total)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwT1tOdm0e84"
      },
      "source": [
        "### Solution\n",
        "Fill out the function below and return the neccessary arguments. Feel free to create more cells if need be"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpqhOqW1UDby"
      },
      "outputs": [],
      "source": [
        "def q2_b(train_data, test_data, dset_id):\n",
        "    \"\"\"\n",
        "    train_data: torch dataset with (n_train, 3, 32, 32) color images as tensors with 256 values rescaled to [0, 1]\n",
        "    test_data: torch dataset with (n_test, 3, 32, 32) color images as tensors with 256 values rescaled to [0, 1]\n",
        "    dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n",
        "               used to set different hyperparameters for different datasets\n",
        "\n",
        "    Returns\n",
        "    - a (# of training iterations, 3) numpy array of full negative ELBO, reconstruction loss E[-p(x|z)],\n",
        "      and KL term E[KL(q(z|x) | p(z))] evaluated every minibatch\n",
        "    - a (# of epochs + 1, 3) numpy array of full negative ELBO, reconstruciton loss E[-p(x|z)],\n",
        "      and KL term E[KL(q(z|x) | p(z))] evaluated once at initialization and after each epoch\n",
        "    - a (100, 32, 32, 3) numpy array of 100 samples from your VAE with values in {0, ..., 255}\n",
        "    - a (100, 32, 32, 3) numpy array of 50 real image / reconstruction pairs\n",
        "      FROM THE TEST SET with values in {0, ..., 255}\n",
        "    - a (100, 32, 32, 3) numpy array of 10 interpolations of length 10 between\n",
        "      pairs of test images. The output should be those 100 images flattened into\n",
        "      the specified shape with values in {0, ..., 255}\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" YOUR CODE HERE \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qvP94pm0hYb"
      },
      "source": [
        "### Results\n",
        "Once you've finished `q2_b`, execute the cells below to visualize and save your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SPtDMRpf0iAG",
        "outputId": "3a31f0be-432d-41d2-d568-a45c4cb49ed6"
      },
      "outputs": [],
      "source": [
        "q2_save_results('b', 1, q2_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QOdX77hP1HMv",
        "outputId": "4e3d95f3-3497-47d1-da53-077da3ec2bed"
      },
      "outputs": [],
      "source": [
        "q2_save_results('b', 2, q2_b)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "16l6wsRW4k8d",
        "pOS0PKRKdtLS"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
