{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEcSNKhrotPo"
      },
      "source": [
        "# Getting Started\n",
        "\n",
        "## General Tips\n",
        "In each homework problem, you will implement various autoencoder models and run them on two datasets (dataset 1 and dataset 2). The expected outputs for dataset 1 are already provided to help as a sanity check.\n",
        "\n",
        "Feel free to print whatever output (e.g. debugging code, training code, etc) you want, as the graded submission will be the submitted pdf with images.\n",
        "\n",
        "After you complete the assignment, download all of the image outputted in the results/ folder and upload them to the figure folder in the given latex template.\n",
        "\n",
        "Run the cells below to download and load up the starter code. It may take longer to run since we are using larger datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get to the parent dir of mai_dul repo\n",
        "import os\n",
        "os.chdir('../../')\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# install latest version deepul package\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from deepul.hw2_helper import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-EZD8GCdx0B"
      },
      "source": [
        "# Question 3: VQ-VAE [40pts] - optional\n",
        "In this question, you with train a [VQ-VAE](https://arxiv.org/abs/1711.00937) on the SVHN and CIFAR10. If you are confused on how the VQ-VAE works, you may find [Lilian Weng's blogpost](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html#vq-vae-and-vq-vae-2) to be useful.\n",
        "\n",
        "You may experiment with different hyperparameters and architecture designs, but the following designs for the VQ-VAE architecture may be useful.\n",
        "\n",
        "```\n",
        "conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "transpose_conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "linear(in_dim, out_dim)\n",
        "batch_norm2d(dim)\n",
        "\n",
        "residual_block(dim)\n",
        "    batch_norm2d(dim)\n",
        "    relu()\n",
        "    conv2d(dim, dim, 3, 1, 1)\n",
        "    batch_norm2d(dim)\n",
        "    relu()\n",
        "    conv2d(dim, dim, 1, 1, 0)\n",
        "\n",
        "Encoder\n",
        "    conv2d(3, 256, 4, 2, 1) 16 x 16\n",
        "    batch_norm2d(256)\n",
        "    relu()\n",
        "    conv2d(256, 256, 4, 2, 1) 8 x 8\n",
        "    residual_block(256)\n",
        "    residual_block(256)\n",
        "\n",
        "Decoder\n",
        "    residual_block(256)\n",
        "    residual_block(256)\n",
        "    batch_norm2d(256)\n",
        "    relu()\n",
        "    transpose_conv2d(256, 256, 4, 2, 1) 16 x 16\n",
        "    batch_norm2d(256)\n",
        "    relu()\n",
        "    transpose_conv2d(256, 3, 4, 2, 1) 32 x 32\n",
        "```\n",
        "\n",
        "A few other tips:\n",
        "*   Use a codebook with $K = 128$ latents each with a $D = 256$ dimensional embedding vector\n",
        "*   You should initialize each element in your $K\\times D$ codebook to be uniformly random in $[-1/K, 1/K]$\n",
        "*   Use batch size 128 with a learning rate of $10^{-3}$ and an Adam optimizer\n",
        "*   Center and scale your images to $[-1, 1]$\n",
        "*   Supposing that $z_e(x)$ is the encoder output, and $z_q(x)$ is the quantized output using the codebook, you can implement the straight-through estimator as follows (where below is fed into the decoder):\n",
        "  * `(z_q(x) - z_e(x)).detach() + z_e(x)` in Pytorch\n",
        "  * `tf.stop_gradient(z_q(x) - z_e(x)) + z_e(x)` in Tensorflow.\n",
        "\n",
        "In addition to training the VQ-VAE, you will also need to train a Transformer prior on the categorical latents in order to sample. Feel free to use your implementation for HW1! You should flatten the VQ-VAE tokens into a [H x W] sequence, and use a start token.\n",
        "\n",
        "**You will provide the following deliverables**\n",
        "\n",
        "\n",
        "1.   Over the course of training, record the average loss of the training data (per minibatch) and test data (for your entire test set) **for both your VQ-VAE and Transformer prior**. Code is provided that automatically plots the training curves.\n",
        "2. Report the final test set performances of your final models\n",
        "3. 100 samples from your trained VQ-VAE and Transformer prior\n",
        "4. 50 real-image / reconstruction pairs (for some $x$, encode and then decode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHZsMrEw5wLN"
      },
      "source": [
        "## Solution\n",
        "Fill out the function below and return the neccessary arguments. Feel free to create more cells if need be"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUQ2V2hLdyUF"
      },
      "outputs": [],
      "source": [
        "def q3(train_data, test_data, dset_id):\n",
        "    \"\"\"\n",
        "    train_data: torch dataset with (n_train, 3, 32, 32) color images as tensors with 256 values rescaled to [0, 1]\n",
        "    test_data: torch dataset with (n_test, 3, 32, 32) color images as tensors with 256 values rescaled to [0, 1]\n",
        "    dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n",
        "               used to set different hyperparameters for different datasets\n",
        "\n",
        "    Returns\n",
        "    - a (# of training iterations,) numpy array of VQ-VAE train losess evaluated every minibatch\n",
        "    - a (# of epochs + 1,) numpy array of VQ-VAE test losses evaluated once at initialization and after each epoch\n",
        "    - a (# of training iterations,) numpy array of Transformer prior train losess evaluated every minibatch\n",
        "    - a (# of epochs + 1,) numpy array of Transformer prior test losses evaluated once at initialization and after each epoch\n",
        "    - a (100, 32, 32, 3) numpy array of 100 samples with values in {0, ... 255}\n",
        "    - a (100, 32, 32, 3) numpy array of 50 real image / reconstruction pairs\n",
        "      FROM THE TEST SET with values in [0, 255]\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" YOUR CODE HERE \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nbn-r53G51X_"
      },
      "source": [
        "## Results\n",
        "Once you've finished `q3`, execute the cells below to visualize and save your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ClKjwiAd535z",
        "outputId": "dc4baa34-e868-4e32-f35d-937f3aab2b9e"
      },
      "outputs": [],
      "source": [
        "q3_save_results(1, q3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vDEvml-59zA"
      },
      "outputs": [],
      "source": [
        "q3_save_results(2, q3)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "16l6wsRW4k8d",
        "pOS0PKRKdtLS"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
