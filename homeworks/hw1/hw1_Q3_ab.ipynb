{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "Follow the general instructions from hw1_Q1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T08:14:18.281474Z",
     "start_time": "2024-11-06T08:14:18.277986Z"
    }
   },
   "source": [
    "# get to the parent dir of mai_dul repo\n",
    "import os\n",
    "\n",
    "# os.chdir('../../')\n",
    "os.getcwd()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ordem/PycharmProjects/MAI_DUL_WS24/homeworks/hw1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T08:14:18.327596Z",
     "start_time": "2024-11-06T08:14:18.326032Z"
    }
   },
   "source": [
    "# run only once at the beginning of working on hw1\n",
    "# !unzip -qq homeworks/hw1/data/hw1_data.zip -d homeworks/hw1/data/\n",
    "\n",
    "# install latest version deepul package\n",
    "# !pip install -e ."
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZHWosWrbpO5Y",
    "ExecuteTime": {
     "end_time": "2024-11-06T08:14:19.347873Z",
     "start_time": "2024-11-06T08:14:18.415397Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from deepul.hw1_helper import (\n",
    "    # Q3\n",
    "    q3ab_save_results,\n",
    ")\n",
    "from torch.optim import Adam\n",
    "from models.IGPTModel import IGPTModel\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMZLcaHwLNNL"
   },
   "source": [
    "# Question 3: Causal Transformer - iGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will move onto the current most popular and widespread autoregressive model, the transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50WsEzhx4Uua"
   },
   "source": [
    "## Part (a) Autoregressive Transformer on Shapes and MNIST\n",
    "In this part, implement a simple Autoregressive Transformer to model binary MNIST and shapes images (same as Q2(a), but with a Transformer). \n",
    "\n",
    "Some additional notes about your transformer implementation:\n",
    " * iGPT uses learned positional encodings. We recommend to use those here as well. However, you may also use sinusoidal positional encodings if you wish (see the [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper)\n",
    " * Autoregressive transformer always predicts the **next** token, give prior tokens. iGPT has a special **\\<bos\\>** or beginning of sequence token at the start of every sequence every image. Make sure to include this in your implementation as well. You can generate unconditional sample by conditioning with the **\\<bos\\>** token.\n",
    " * While dropout is a common feature in transformer models, you do not need to add it (but may if you wish!).\n",
    " * Prebuilt transformers exist in some frameworks (i.e. pytorch). Don't just use an off the shelf implementation as the point of the exercise is to better understand the transformer architecture. Building the transformer from the ground up (use primitives such as Linear/Dense layers, LayerNorm, GeLU, Embedding)\n",
    " * Learning rate warmup and cos learning rate decay are often used when training transformers to improve training stability and improve performance. See if this helps your model! Try 1000 steps of warmup with a cosine learning rate decay.\n",
    "\n",
    "Paper references\n",
    "* [Attention Is All You Need](https://arxiv.org/abs/1706.03762) \n",
    "* [Generative Pretraining from Pixels](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf) \n",
    "* [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "We recommend the following network design parameters:\n",
    "* $d_{model}$: 128\n",
    "* heads: 4\n",
    "* layers: 2\n",
    "* GeLU nonlinearities\n",
    "\n",
    "And the following hyperparameters:\n",
    "* Batch size: 64 or 32 or 16 (whichever fits in your GPU)\n",
    "* Learning rate: $10^{-3}$\n",
    "* 15 epochs or more\n",
    "* Adam Optimizer (this applies to all Transformers models trained in future parts)\n",
    "\n",
    "**You will provide these deliverables**\n",
    "\n",
    "1. Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n",
    "2. Report the final test set performance of your final model\n",
    "3. 100 samples from the final trained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T08:14:19.358474Z",
     "start_time": "2024-11-06T08:14:19.352418Z"
    }
   },
   "source": [
    "def q3_a(train_data, test_data, image_shape, dset_id):\n",
    "    \"\"\"\n",
    "    train_data: A (n_train, H, W, 1) uint8 numpy array of color images with values in {0, 1}\n",
    "    test_data: A (n_test, H, W, 1) uint8 numpy array of color images with values in {0, 1}\n",
    "    image_shape: (H, W, 1), height, width, and # of channels of the image\n",
    "    dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n",
    "           used to set different hyperparameters for different datasets\n",
    "\n",
    "    Returns\n",
    "    - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n",
    "    - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n",
    "    - a numpy array of size (100, H, W, 1) of samples with values in {0, 1}\n",
    "    \"\"\"\n",
    "    batch_size = 128\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    max_epochs = 1\n",
    "\n",
    "    train_dataset = TensorDataset(torch.tensor(train_data).permute((0, 3, 1, 2)).float())\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    def transform(x):\n",
    "        x_flat = torch.flatten(x, start_dim=1)\n",
    "        return torch.cat((2 * torch.ones(x_flat.shape[0], 1).to(x_flat.device), x_flat), dim=1).long()\n",
    "\n",
    "    x_test = transform(torch.tensor(test_data).permute((0, 3, 1, 2)).float())\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    test_dataset = TensorDataset(x_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = IGPTModel(3, max_sequence_length=x_test.shape[-1])\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for epoch in range(max_epochs):\n",
    "        batches = iter(train_loader)\n",
    "\n",
    "        batch_train_loss = np.empty(len(batches))\n",
    "        for i, [batch_x] in enumerate(batches):\n",
    "            batch_x = transform(batch_x.to(device))\n",
    "            logits = model.forward(batch_x)\n",
    "\n",
    "            loss = model.loss_function(logits, batch_x)\n",
    "            batch_train_loss[i] = loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        train_losses.append(batch_train_loss.mean().item())\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_test_loss = np.empty(len(batches))\n",
    "            for i, [test_batch_x] in enumerate(test_loader):\n",
    "                test_batch_x=test_batch_x.to(device)\n",
    "                test_logits = model.forward(test_batch_x)\n",
    "                test_loss = model.loss_function(test_logits, test_batch_x)\n",
    "                batch_test_loss[i] = test_loss.item()\n",
    "            test_losses.append(batch_train_loss.mean())\n",
    "        model.train()\n",
    "        print(f\"epoch {epoch + 1}/{max_epochs}, train loss: {train_losses[-1]}, test loss: {test_losses[-1]}\")\n",
    "    samples = model.generate(100, device).permute((0, 2, 3, 1)).numpy(force=True)\n",
    "    return train_losses, test_losses, samples"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGp2OsLKiToN"
   },
   "source": [
    "### Results\n",
    "\n",
    "Once you've implemented `q3_a`, execute the cells below to visualize and save your results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "kW-k-59qJaKN",
    "outputId": "78b08f6a-12db-46b7-ac27-a07693a9cd09",
    "ExecuteTime": {
     "end_time": "2024-11-06T08:14:19.403300Z",
     "start_time": "2024-11-06T08:14:19.401207Z"
    }
   },
   "source": "q3ab_save_results(1, 'a', q3_a)",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "It_iPXaZjlk0",
    "outputId": "3ba138b1-71c9-4f67-8413-b5ef23db8204",
    "ExecuteTime": {
     "end_time": "2024-11-06T08:18:27.542224Z",
     "start_time": "2024-11-06T08:14:19.449689Z"
    }
   },
   "source": "q3ab_save_results(2, 'a', q3_a)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/25, train loss: 0.19090554092738674, test loss: 0.19090554092738674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50WsEzhx4Uua"
   },
   "source": [
    "## Part (b) iGPT on Colored Shapes and MNIST\n",
    "\n",
    "Now, implement an iGPT that models color. In order to reduce the length of token sequences, iGPT models each RGB pixel as a **single** token. This effectively reduces the context length from H*W*C to just H*W. iGPT does this through a k-means clustering approach. Because our images only each can only take on 4 values (2 bits) per channel, we can represent each pixel with 64 values (6 bits). Convert the dataset into an image of tokens and train iGPT on the colored shapes and MNIST dataset.\n",
    "\n",
    "Checkout the iGPT paper for more details: [Generative Pretraining from Pixels](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf) \n",
    "\n",
    "Training times and hyperparameter settings should be the same as part (a), except train for longer (15 epochs)\n",
    "\n",
    "**You will provide these deliverables**\n",
    "\n",
    "1.   Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n",
    "2.   Report the final test set performance of your final model\n",
    "3. 100 samples from the final trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T08:18:27.621074201Z",
     "start_time": "2024-11-06T07:58:34.608861Z"
    }
   },
   "source": [
    "from models.ColorIGPT import ColorIGPTModel\n",
    "\n",
    "\n",
    "def q3_b(train_data, test_data, image_shape, dset_id):\n",
    "    \"\"\"\n",
    "    train_data: A (n_train, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n",
    "    test_data: A (n_test, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n",
    "    image_shape: (H, W, C), height, width, and # of channels of the image\n",
    "    dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n",
    "             used to set different hyperparameters for different datasets\n",
    "  \n",
    "    Returns\n",
    "    - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n",
    "    - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n",
    "    - a numpy array of size (100, H, W, C) of samples with values in {0, 1, 2, 3}\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = 128\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    max_epochs = 25\n",
    "\n",
    "    train_dataset = TensorDataset(torch.tensor(train_data).permute((0, 3, 1, 2)).float())\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    def transform(x):\n",
    "        x_flat = torch.flatten(x, start_dim=2)\n",
    "        return torch.cat((2 * torch.ones(x_flat.shape[0], 1).to(x_flat.device), x_flat), dim=1).long()\n",
    "\n",
    "    x_test = transform(torch.tensor(test_data).permute((0, 3, 1, 2)).float())\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    x_test = x_test.to(device)\n",
    "\n",
    "    model = ColorIGPTModel(5, max_sequence_length=20 * 20 + 1)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for epoch in range(max_epochs):\n",
    "        batches = iter(train_loader)\n",
    "\n",
    "        batch_train_loss = np.empty(len(batches))\n",
    "        for i, [batch_x] in enumerate(batches):\n",
    "            batch_x = transform(batch_x.to(device))\n",
    "            logits = model.forward(batch_x)\n",
    "\n",
    "            loss = model.loss_function(logits, batch_x)\n",
    "            batch_train_loss[i] = loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        train_losses.append(batch_train_loss.mean().item())\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_logits = model.forward(x_test)\n",
    "            test_loss = model.loss_function(test_logits, x_test)\n",
    "            test_losses.append(test_loss.item())\n",
    "        model.train()\n",
    "        print(f\"epoch {epoch + 1}/{max_epochs}, train loss: {train_losses[-1]}, test loss: {test_losses[-1]}\")\n",
    "    samples = model.generate(100, device).permute((0, 2, 3, 1)).numpy(force=True)\n",
    "    return train_losses, test_losses, samples"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGp2OsLKiToN"
   },
   "source": [
    "### Results\n",
    "\n",
    "Once you've implemented `q3_b`, execute the cells below to visualize and save your results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "kW-k-59qJaKN",
    "outputId": "78b08f6a-12db-46b7-ac27-a07693a9cd09",
    "ExecuteTime": {
     "end_time": "2024-11-06T08:18:27.621420760Z",
     "start_time": "2024-11-06T07:58:34.668142Z"
    }
   },
   "source": [
    "q3ab_save_results(1, 'b', q3_b)"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 3",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mq3ab_save_results\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mq3_b\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/MAI_DUL_WS24/homeworks/hw1/deepul/hw1_helper.py:208\u001B[0m, in \u001B[0;36mq3ab_save_results\u001B[0;34m(dset_type, part, fn)\u001B[0m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m()\n\u001B[0;32m--> 208\u001B[0m train_losses, test_losses, samples \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimg_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdset_type\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    209\u001B[0m samples \u001B[38;5;241m=\u001B[39m samples\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfloat32\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m/\u001B[39m channel \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m255\u001B[39m\n\u001B[1;32m    211\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFinal Test Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_losses[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[17], line 30\u001B[0m, in \u001B[0;36mq3_b\u001B[0;34m(train_data, test_data, image_shape, dset_id)\u001B[0m\n\u001B[1;32m     27\u001B[0m     x_flat \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mflatten(x, start_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat((\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m torch\u001B[38;5;241m.\u001B[39mones(x_flat\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mto(x_flat\u001B[38;5;241m.\u001B[39mdevice), x_flat), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mlong()\n\u001B[0;32m---> 30\u001B[0m x_test \u001B[38;5;241m=\u001B[39m \u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_data\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpermute\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     32\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     33\u001B[0m x_test \u001B[38;5;241m=\u001B[39m x_test\u001B[38;5;241m.\u001B[39mto(device)\n",
      "Cell \u001B[0;32mIn[17], line 28\u001B[0m, in \u001B[0;36mq3_b.<locals>.transform\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtransform\u001B[39m(x):\n\u001B[1;32m     27\u001B[0m     x_flat \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mflatten(x, start_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m---> 28\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mones\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_flat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_flat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_flat\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mlong()\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Tensors must have same number of dimensions: got 2 and 3"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "It_iPXaZjlk0",
    "outputId": "3ba138b1-71c9-4f67-8413-b5ef23db8204"
   },
   "outputs": [],
   "source": [
    "q3ab_save_results(2, 'b', q3_b)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": " Homework 1 Autoregressive Models (Solutions).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
